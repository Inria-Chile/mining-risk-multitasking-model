# -*- coding: utf-8 -*-
"""MultiTaskLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DQv9LDO-CAO6SJmgG25G3Rv9VrOhpERu

# Multi Task Learning

A continuación se diseñará e implementará una red neuronal en configuración de *multi task learning* para resolver las siguientes tareas:

1. Clasificar si una faena tendrá accidentes en el futuro.
Esta probabilidad es interpretable como un potencial de accidentabilidad.
2. Predecir el número de días hasta el siguiente accidente

Luego, se pueden definir dos funciones de pérdida.
En primer lugar, aquella relacionada con el problema de clasificación, que corresponderá a una entropía cruzada entre las etiquetas y lo inferido.
Luego, aquella relacionada con el problema de regresión, que corresponderá al error cuadrático medio entre los valores observados y los inferidos.
Este último término solo es considerado para las faenas que presentan accidentes en el futuro.

Se implementará con la librería PyTorch.
"""

!nvidia-smi

# Mount Google Drive shared drive
from google.colab import drive
drive.mount("/content/gdrive")

# Install relevant libraries
!pip install torch torchvision
!pip install matplotlib seaborn
!pip install comet_ml

"""## Dataset

A continuación se Define la clase `WorksitesDataset`, que implementa un `torch.utils.data.Dataset`.
Esencialmente, es responsable de consumir el conjunto de datos contenido en el archivo `worksites.csv`.
"""

BASE_PATH = "/content/gdrive/Shared drives/ECML-PKDD-2020"

import pandas as pd
import numpy as np

from comet_ml import Experiment, ExistingExperiment

from sklearn.preprocessing import MinMaxScaler, RobustScaler

import torch
from torch.utils.data import Dataset


class WorksitesDataset(Dataset):
    
    TRAIN = "train"
    VAL = "val"
    TEST = "test"
    SPLIT_FRACTIONS = {
        TRAIN: 0.7,
        VAL: 0.1,
        TEST: 0.2,
    }
    
    def __init__(self, split, label_columns, csv_path, feature_scaler=None, scale_features=True, label_scaler=None, scale_labels=False):
        """
        Class constructor.
        Args:
            - split: the split that this WorksitesDataset instance will process
            - labels_column: the labels column name
            - csv_path: the path to the dataset file
            - scaler: a scikit-learn scaler to scale the features, if None a MinMaxScaler is instantiated
            - scale_features: whether to scale the features or not
        """
        # Sanity check
        assert split in (self.TRAIN, self.VAL, self.TEST)
        
        self._csv_path = csv_path
        self._split = split
        self._label_columns = label_columns
        self._feature_scaler = feature_scaler
        self._scale_features = scale_features
        self._label_scaler = label_scaler
        self._scale_labels = scale_labels
        self._df = None
        self._features_df = None
        self._labels_df = None
        
        
        self._load_dataset()
    
    @staticmethod
    def _df_to_features_labels(df, label_columns, scale_features, scale_labels, feature_scaler=None, label_scaler=None):
        """
        Creates features and labels of a DataFrame.
        Args:
            - df: a DataFrame
        Returns:
            - features: a DataFrame with features
            - labels: a Series with labels
            - feature_scaler: the scikit-learn scaler applied onto the features
        """
        # Get features.
        # Also, get `labels_columns` values which will be used when training the model.
        features = df[[
            "NUM_FACILITIES",
            "DAYS_SINCE_LAST_INSPECTION",
            "STOPPED_BY_SANCTION",
            "PENDING_ACTIONS",
            "NO_TIME_LOST_COUNT",
            "TIME_LOST_COUNT",
            "FATAL_COUNT",
            "FATAL_TIME_LOST_ACCIDENTS_COUNT",
            "TOTAL_ACCIDENTS_COUNT",
            "HOURS_WORKED",
            "ACCIDENTS_RATE",
        ]].copy()
        labels = df[label_columns].copy()

        # Build dummy variables de MONTH
        df["MONTH"] = pd.Categorical(df["MONTH"], categories=range(1, 13))
        dummy_month = pd.get_dummies(df["MONTH"], prefix="MONTH")
        features = features.join(dummy_month)

        # Build HAS_NEVER_BEEN_INSPECTED variable
        has_never_been_inspected = ~np.isfinite(features["DAYS_SINCE_LAST_INSPECTION"])
        features["HAS_NEVER_BEEN_INSPECTED"] = has_never_been_inspected.astype(int)
        features.loc[has_never_been_inspected, "DAYS_SINCE_LAST_INSPECTION"] = 0

        # Scale features to the same numeric range, preserving values distribution
        if scale_features:
            if feature_scaler is None:
                feature_scaler = MinMaxScaler()
                features[features.columns] = feature_scaler.fit_transform(features[features.columns])
            else:
                features[features.columns] = feature_scaler.transform(features[features.columns])

        if scale_labels:
            if label_scaler is None:
                label_scaler = RobustScaler()
                labels = label_scaler.fit_transform(labels)
            else:
                labels = label_scaler.transform(labels)
        else:
            labels = labels.to_numpy()

        return features, labels, feature_scaler, label_scaler
    
    @staticmethod
    def _preprocess_df(df):
        # Fill None and NaNs with sensible values
        df["LAST_INSPECTION_DATE"].fillna(-np.inf, inplace=True)
        df["LAST_INSPECTION_YEAR"].fillna(-np.inf, inplace=True)
        df["DAYS_SINCE_LAST_INSPECTION"].fillna(np.inf, inplace=True)
        # df["DAYS_UNTIL_NEXT_ACCIDENT"].fillna(np.inf, inplace=True)
        df["HOURS_WORKED"].fillna(0, inplace=True)
        
        # Create new columns with accidents sums
        df["TOTAL_ACCIDENTS_COUNT"] = df["NO_TIME_LOST_COUNT"] + df["TIME_LOST_COUNT"] + df["FATAL_COUNT"]
        df["FATAL_TIME_LOST_ACCIDENTS_COUNT"] = df["TIME_LOST_COUNT"] + df["FATAL_COUNT"]
        
        return df
    

    def _load_dataset(self):
        df = pd.read_csv(self._csv_path)
        
        # Get the dataset split
        df = df.sort_values(by=["YEAR", "MONTH", "WORKSITE_ID"], ascending=[True, True, True])
        train_idxs = (0, int(self.SPLIT_FRACTIONS[self.TRAIN] * len(df)))
        val_idxs = (train_idxs[1], train_idxs[1] + int(self.SPLIT_FRACTIONS[self.VAL] * len(df)))
        test_idxs = (val_idxs[1], val_idxs[1] + int(self.SPLIT_FRACTIONS[self.TEST] * len(df)))
        splits_idxs = {
            self.TRAIN: train_idxs,
            self.VAL: val_idxs,
            self.TEST: test_idxs,
        }
        split_idxs = splits_idxs[self._split]
        split_df = df.iloc[split_idxs[0]:split_idxs[1]].copy()
        
        split_df = self._preprocess_df(split_df)
        self._features_df, self._labels_df, self._feature_scaler, self._label_scaler = self._df_to_features_labels(
            df=split_df,
            label_columns=self._label_columns,
            scale_features=self._scale_features,
            scale_labels=self._scale_labels,
            feature_scaler=self._feature_scaler,
            label_scaler=self._label_scaler
        )
        
        self._df = split_df
    
    def __getitem__(self, idx):
        features = self._features_df.iloc[idx].values
        classifier_target, regressor_target = self._labels_df[idx]
        return {
            "features": torch.tensor(features),
            "classifier_target": torch.tensor(classifier_target > 0, dtype=torch.int64),
            "regressor_target": torch.tensor(regressor_target)
        }
    
    def __len__(self):
        return len(self._features_df)

from torch.utils.data import DataLoader

HYPERPARAMETERS = {
    "batch_size": 32,
    "input_size": 24,
    "hidden_size": 50,
    "learning_rate": 0.0005,
    "epochs": 150,
    "multitasking_enabled": True,
    "lr_scheduler_enabled": False,
    "lr_scheduler_factor": 0.5,
    "lr_scheduler_patience": 5,
    "lr_scheduler_verbose": True,
}

LABEL_COLUMNS = ["FUTURE_TOTAL_COUNT", "DAYS_UNTIL_NEXT_ACCIDENT"]
DATASET_PATH = f"{BASE_PATH}/datasets/worksites.csv"

train_ds = WorksitesDataset(
    csv_path=DATASET_PATH,
    split=WorksitesDataset.TRAIN,
    label_columns=LABEL_COLUMNS
)
val_ds = WorksitesDataset(
    csv_path=DATASET_PATH,
    split=WorksitesDataset.VAL,
    label_columns=LABEL_COLUMNS,
    feature_scaler=train_ds._feature_scaler,
    label_scaler=train_ds._label_scaler
)
test_ds = WorksitesDataset(
    csv_path=DATASET_PATH,
    split=WorksitesDataset.TEST,
    label_columns=LABEL_COLUMNS,
    feature_scaler=train_ds._feature_scaler,
    label_scaler=train_ds._label_scaler
)

train_dl = DataLoader(
    train_ds,
    batch_size=HYPERPARAMETERS["batch_size"],
    shuffle=True
)
val_dl = DataLoader(
    val_ds,
    batch_size=HYPERPARAMETERS["batch_size"],
    shuffle=False
)
test_dl = DataLoader(
    test_ds,
    batch_size=HYPERPARAMETERS["batch_size"],
    shuffle=False
)

from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error

import torch.nn as nn


class MultiTaskLearner(nn.Module):
    def __init__(self, input_size, hidden_size, device):
        super().__init__()

        self.device = device
        
        self.hidden_fc = nn.Linear(
            in_features=input_size,
            out_features=hidden_size,
            bias=True
        )
        self.tanh = nn.Tanh()
        self.classifier_fc = nn.Linear(
            in_features=hidden_size,
            out_features=2, # It's a binary classification
            bias=True
        )
        self.regressor_fc = nn.Linear(
            in_features=hidden_size,
            out_features=1, # It's a regression
            bias=True
        )
    
    def forward(self, features):
        hidden_features = self.hidden_fc(features)
        hidden_features = self.tanh(hidden_features)
        classification = self.classifier_fc(hidden_features)
        regression = self.regressor_fc(hidden_features)
        return classification, regression
        
    def loss(self, classification_predicted, regression_predicted, classification_target, regression_target):
        """
        Args:
            - classification_predicted: tensor with shape [batch_sz, 2]
            - regression_predicted: tensor with shape [batch_sz]
            - classification_target: tensor with shape [batch_sz]
            - regression_target: tensor with shape [batch_sz]
        Returns:
            - loss
            - loss_metrics
        """
        positive_classes_count = classification_target.sum().unsqueeze(0).float()
        negative_classes_count = (torch.tensor(classification_target.size()).to(self.device) - positive_classes_count).float()
        classification_criterion = nn.CrossEntropyLoss(weight=torch.cat([positive_classes_count, negative_classes_count], dim=0))
        regression_criterion = nn.MSELoss()
        classification_loss = classification_criterion(classification_predicted, classification_target)
        
        regression_mask = torch.isnan(regression_target)
        regression_predicted[regression_mask] = 0
        regression_target[regression_mask] = 0
        regression_loss = regression_criterion(regression_predicted.squeeze(), regression_target)
        
        loss = classification_loss + (regression_loss if HYPERPARAMETERS["multitasking_enabled"] else 0)
        return loss, {
            "loss": loss.item(),
            "classification_loss": classification_loss.item(),
            "regression_loss": regression_loss.item(),
        }
    
    @staticmethod
    def mask_regressor_tensors(target, predicted):
        mask = ~torch.isfinite(target)
        predicted[mask] = 0
        target[mask] = 0
        return target, predicted
    
    @staticmethod
    def classification_metrics(target, predicted):
        precision = precision_score(target, predicted)
        recall = recall_score(target, predicted)
        f1 = f1_score(target, predicted)
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
        }
    
    @staticmethod
    def regression_metrics(target, predicted):
        mse = mean_squared_error(target, predicted)
        return {
            "mse": mse,
        }

device = "cuda" if torch.cuda.is_available() else "cpu"
net = MultiTaskLearner(
    input_size=HYPERPARAMETERS["input_size"],
    hidden_size=HYPERPARAMETERS["hidden_size"],
    device=device
)
net.to(device)
net, device

optimizer = torch.optim.SGD(net.parameters(), lr=HYPERPARAMETERS["learning_rate"])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer=optimizer,
    mode="min",
    factor=HYPERPARAMETERS["lr_scheduler_factor"],
    patience=HYPERPARAMETERS["lr_scheduler_patience"],
    verbose=HYPERPARAMETERS["lr_scheduler_verbose"],
)
optimizer, scheduler

"""## Entrenamiento"""

from collections import defaultdict

from tqdm import tqdm

def run_epoch(dl, model, device, should_train, optimizer=None):
    """
    Args:
        - dl: a DataLoader
        - model: a PyTorch nn.Module
        - should_train: should the loop backpropagate or not
        - device: the device name where tensors are going to be sent
    """
    metrics = defaultdict(list)
    classifier_targets = []
    classifier_predictions = []
    regressor_targets = []
    regressor_predictions = []
    for batch in (dl):
        features = batch["features"].float().to(device)
        classifier_target = batch["classifier_target"].long().to(device)
        regressor_target = batch["regressor_target"].float().to(device)
        
        # Inference
        classifier_predicted, regressor_predicted = model(features)
        
        # Calculate loss, loss metrics, and save them
        loss, loss_metrics = model.loss(classifier_predicted, regressor_predicted, classifier_target, regressor_target)
        for key, value in loss_metrics.items():
            metrics[key].append(value)
            
        
        # Backpropagate if needed
        if should_train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Save classification targets and predictions
        classifier_targets += classifier_target.tolist()
        classifier_predictions += classifier_predicted.argmax(dim=1).tolist()
        
        # Save regression targets and predictions
        regressor_target, regressor_predicted = model.mask_regressor_tensors(regressor_target, regressor_predicted)
        regressor_targets += regressor_target.tolist()
        regressor_predictions += regressor_predicted.squeeze().tolist()
        
    metrics = {
        **model.classification_metrics(classifier_targets, classifier_predictions),
        **model.regression_metrics(regressor_targets, regressor_predictions),
        **metrics,
    }
    
    mean_metrics = {}
    for key, value in metrics.items():
        mean_metrics[f"mean_{key}"] = np.mean(value)
        
    return mean_metrics

def load_last_model(model, optimizer, scheduler):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dump_dict = torch.load(f"{BASE_PATH}/models/last.pth")
    epoch = dump_dict["epoch"]
    print(f"Resuming model at {epoch} epoch...")
    model.load_state_dict(dump_dict["model_state_dict"])
    optimizer.load_state_dict(dump_dict["optimizer_state_dict"])
    scheduler.load_state_dict(dump_dict["scheduler_state_dict"])
    return dump_dict

def load_best_model(dump_name="best"):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    best_dump = torch.load(f"{BASE_PATH}/models/{dump_name}.pth")
    best_epoch = best_dump["epoch"]
    print(f"Best model was at {best_epoch} epoch...")
    model = MultiTaskLearner(
        input_size=HYPERPARAMETERS["input_size"],
        hidden_size=HYPERPARAMETERS["hidden_size"],
        device=device
    )
    model.to(device)
    model.load_state_dict(best_dump["model_state_dict"])
    return model, best_epoch

def train_model(model, optimizer, scheduler, train_dl, val_dl, test_dl, epochs, val_target_metric, device, load_best_model_fn, should_resume=False, load_last_model_fn=None):

    if should_resume:
        checkpoint_dict = load_last_model_fn(model, optimizer, scheduler)
        experiment = ExistingExperiment(api_key="GnSgLcpQFJJo8uM2tJ9LKhNHl", previous_experiment=checkpoint_dict["experiment_key"])
        train_metrics = checkpoint_dict["train_metrics"]
        val_metrics = checkpoint_dict["val_metrics"]
        best_score = min(checkpoint_dict["best_score"], checkpoint_dict["current_score"])
        epoch_offset = checkpoint_dict["epoch"]
    else:
        experiment = Experiment(api_key="GnSgLcpQFJJo8uM2tJ9LKhNHl")
        experiment.log_parameters(HYPERPARAMETERS)
        train_metrics = defaultdict(list)
        val_metrics = defaultdict(list)
        best_score = float("inf") # NB. This assumes the metric will descend
        epoch_offset = 0
    
    for epoch in tqdm(range(1 + epoch_offset, epochs + 1)):
        train_epoch_metrics = run_epoch(train_dl, model, device, should_train=True, optimizer=optimizer)
        val_epoch_metrics = run_epoch(val_dl, model, device, should_train=False)
        with experiment.train():
            experiment.log_metrics(train_epoch_metrics, epoch=epoch)
        with experiment.validate():
            experiment.log_metrics(val_epoch_metrics, epoch=epoch)

        for key, value in train_epoch_metrics.items():
            train_metrics[key].append(value)
        for key, value in val_epoch_metrics.items():
            val_metrics[key].append(value)

        current_score = val_epoch_metrics[val_target_metric]
        dump_contents_dict = {
            "train_metrics": train_metrics,
            "val_metrics": val_metrics,
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict(),
            "best_score": best_score,
            "current_score": current_score,
            "experiment_key": experiment.get_key(),
        }
        torch.save(dump_contents_dict, f"{BASE_PATH}/models/last.pth")
        
        if current_score < best_score:
            best_score = current_score
            torch.save(dump_contents_dict, f"{BASE_PATH}/models/best.pth")

        if HYPERPARAMETERS["lr_scheduler_enabled"]:
            scheduler.step(current_score)
        
    
    # Evaluate best model on the test set
    best_model, best_epoch = load_best_model_fn()
    best_model.to(device)
    test_metrics = run_epoch(train_dl, best_model, device, should_train=False)
    with experiment.test():
        experiment.log_metrics(test_metrics, epoch=best_epoch)
    
    experiment.end()

    return train_metrics, val_metrics, test_metrics

train_metrics, val_metrics, test_metrics = train_model(
    model=net,
    optimizer=optimizer,
    scheduler=scheduler,
    train_dl=train_dl,
    val_dl=val_dl,
    test_dl=test_dl,
    epochs=HYPERPARAMETERS["epochs"],
    val_target_metric="mean_loss",
    device=device,
    load_best_model_fn=load_best_model,
    should_resume=False,
    load_last_model_fn=load_last_model
)

test_metrics

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()

plots = [
    ("Precision", "mean_precision"),
    ("Recall", "mean_recall"),
    ("F1", "mean_f1"),
    ("MSE", "mean_mse"),
    ("Loss", "mean_loss"),
    ("Classification loss", "mean_classification_loss"),
    ("Regression loss", "mean_regression_loss"),
]

for plot in plots:
    title, key = plot
    plt.figure(figsize=(12, 8))
    sns.lineplot(x=range(1, HYPERPARAMETERS["epochs"] + 1), y=train_metrics[key], label="Training")
    ax = sns.lineplot(x=range(1, HYPERPARAMETERS["epochs"] + 1), y=val_metrics[key], label="Validation")
    ax.set(
        xlabel="Epoch",
        ylabel=title,
        title=f"{title} v/s epoch"
    )

"""## Evaluación

A continuación se carga el mejor modelo entrenado.
El mejor modelo entrenado corresponde a aquel que minimiza la pérdida en el conjunto de validación.

Cada faena será ordenada de acuerdo a la probabilidad de sufrir accidente en el futuro.
Luego, usando la métrica de NDCG se comparará al modelo de expertos.
"""

device = "cuda" if torch.cuda.is_available() else "cpu"
net, _ = load_best_model(dump_name="best")
net.to(device)
net

REGIONAL_OFFICES = {
    "ARICA": (1376,),
    "TARAPACA": (1363,),
    "ANTOFAGASTA": (1364,),
    "ATACAMA": (1365,),
    "COQUIMBO": (1366,),
    "CENTRO": (1367, 1374),
    "O'HIGGINS": (1362,),
    "MAULE": (1368,),
    "SUR": (1369, 1372, 1371, 1375, 1370,),
    "MAGALLANES": (1373,),    
}


def dcg(df, p, relevance_column):
    """
    Computes the discounted cumulative gain of a DataFrame, according to some relevance.
    Ref.: https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Discounted_Cumulative_Gain
    Args:
        - df: a DataFrame
        - p: last position to include
        - relevance_column: name of the column with each row's column score
    Returns:
        - the discounted cumulative gain
    """
    def _dcg(args):
        (i, (_, row)) = args
        num = row[relevance_column]
        den = np.log2(i + 2) # First `i` is zero
        return num / den
    return sum(map(_dcg, enumerate(df[:p].iterrows())))


def ndcg(df, p, relevance_column):
    """
    Computes the normalized discounted cumulative gain of a DataFrame, according to some relevance.
    Ref.: https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG
    Args:
        - df: a DataFrame
        - p: last position to include
        - relevance_column: name of the column with each row's column score
    Returns:
        - the normalized discounted cumulative gain
    """
    ideal = df.sort_values(by=relevance_column, ascending=False)
    num = dcg(df, p, relevance_column)
    den = dcg(ideal, p, relevance_column)
    return num / den


def evaluate_region(df, ds_class, region_ids, model, relevance_column, scale_features, feature_scaler, scale_labels, label_scaler, device):
    """
    Args:
        - df: a DataFrame
        - ds_class: a Dataset class implementing required static methods
        - region_ids: sequence of region identifiers
        - model: a PyTorch nn.Module
        - relevance_column: column label with record relevance
        - scale_features: whether to scale the features or not
        - scaler: a scikit-learn scaler to scale the features, if scale_features is True
    Returns:
        - ndcg: normalized discounted cumulative gain
    """
    print("Region IDs: ", region_ids)
    results = []
    for year_month, group_df in df.groupby(["YEAR", "MONTH"]):
        features, labels_series, _, _ = ds_class._df_to_features_labels(
            df=group_df.copy(),
            label_columns=relevance_column,
            scale_features=scale_features,
            feature_scaler=feature_scaler,
            scale_labels=False,
            label_scaler=label_scaler
        )
        
        # If there is no relevance, go on to the next iteration
        if labels_series.sum() == 0 or len(labels_series) == 0:
            display(f"Skipping: {region_ids}/{year_month}... No relevance...")
            continue
        
        features = torch.tensor(features.values).float().to(device)
        classifier_predicted, _ = model(features)
        criticality = torch.softmax(classifier_predicted, dim=1)[:,1]
        
        criticality_df = pd.DataFrame(
            criticality.detach(),
            index=labels_series.index,
            columns=["CRITICALITY"]
        )
        joined = criticality_df.join(labels_series)
        ranked = joined.sort_values(by="CRITICALITY", ascending=False)
        score = ndcg(ranked, len(ranked), relevance_column)
        
        if not np.isnan(score):
            results.append(score)
        else:
            display(f"Skipping: {region_ids}/{year}/{month}... NaN...")
            
    results = np.array(results)
    return {
        "mean": np.mean(results),
        "median": np.median(results),
    }


def evaluate_model(df, ds_class, model, relevance_column, scale_features, feature_scaler, scale_labels, label_scaler, device):
    preprocessed_df = ds_class._preprocess_df(df)
    results = []
    for region_name, region_ids in REGIONAL_OFFICES.items():
        region_filter = preprocessed_df["REGION_ID"].isin(region_ids)
        filtered_df = preprocessed_df[region_filter]
        result = evaluate_region(df=filtered_df,
                                 ds_class=ds_class,
                                 region_ids=region_ids,
                                 model=model,
                                 relevance_column=relevance_column,
                                 scale_features=scale_features,
                                 feature_scaler=feature_scaler,
                                 scale_labels=scale_labels,
                                 label_scaler=label_scaler,
                                 device=device)
        results.append({"region": region_name, **result})
    return results


ACCIDENTS_TWELVE_MONTHS = "ACCIDENTS_TWELVE_MONTHS"

results = evaluate_model(
    df=test_ds._df,
    ds_class=WorksitesDataset,
    model=net,
    relevance_column=ACCIDENTS_TWELVE_MONTHS,
    scale_features=True,
    feature_scaler=train_ds._feature_scaler,
    scale_labels=True,
    label_scaler=train_ds._label_scaler,
    device=device)
results = pd.DataFrame(results)
results

def display_experiment_results(results):
    mean = results["mean"]
    median = results["median"]
    print(f"Mean mean:   {mean.mean()}")
    print(f"Median mean: {median.mean()}", end="\n\n")

display_experiment_results(results)

"""## Single batch overfitting"""

if False:
    sample_batch = next(iter(train_dl))
    period = 2000
    for i in range(1, 500000):
        features = sample_batch["features"].float()
        classifier_target = sample_batch["classifier_target"]
        regressor_target = sample_batch["regressor_target"]
        classifier_predicted, regressor_predicted = net(features)
        loss = net.loss(classifier_predicted, regressor_predicted, classifier_target, regressor_target)
        if i % period == 0:
            print(f"Epoch {i}: {loss.item()}")
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()